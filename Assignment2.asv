%% HUMAN SHADOW ROBOT
% 41013 Robotics - Autumn 2020
% Developed by : 
% Ehsan Mahmood & Mohammad Talha Tariq
% Brief:
% This was an attempt to try developing a system that includes a 7DOF Robot arm that 
% can be controlled by the movement of a human hand using computer Vision
% Used Hardware: 
% ## Han's Cute 7DOF Robot (Previously Known as Cyton 300e)
% ## Xbox Kinect V2

%% Code Starts From Here
clear all
clc
clf


%% Computer Vision Algorithm        [Taken From: 
imaqreset;
Environment();
Cyton = UR10;
% create color and depth kinect videoinput objects
colorVid = videoinput('kinect', 1);
depthVid = videoinput('kinect', 2);
triggerconfig (depthVid,'manual');
framesPerTrig = 1;
depthVid.FramesPerTrigger=framesPerTrig;
depthVid.TriggerRepeat=inf;
src = getselectedsource(depthVid);
src.EnableBodyTracking = 'on'; 

start(depthVid);

himg = figure;

prevPos = zeros(1,3);
SkeletonConnectionMap = [ [4 3];  % Neck
                          [3 21]; % Head
                          [21 2]; % Right Leg
                          [2 1];
                          [21 9];
                          [9 10];  % Hip
                          [10 11];
                          [11 12]; % Left Leg
                          [12 24];
                          [12 25];
                          [21 5];  % Spine
                          [5 6];
                          [6 7];   % Left Hand
                          [7 8];
                          [8 22];
                          [8 23];
                          [1 17];
                          [17 18];
                          [18 19];  % Right Hand
                          [19 20];
                          [1 13];
                          [13 14];
                          [14 15];
                          [15 16];
                        ];
while ishandle(himg)
trigger(depthVid)
[depthMap, ts, depthMetaData] = getdata (depthVid);

anyBodiesTracked = any(depthMetaData.IsBodyTracked ~= 0);
trackedBodies = find(depthMetaData.IsBodyTracked);
nBodies = length(trackedBodies);
colors = ['g';'r';'b';'c';'y';'m'];
imshow (depthMap, [0 4096]);

if  sum(depthMetaData.IsBodyTracked) >0
skeletonJoints = depthMetaData.DepthJointIndices (:,:,depthMetaData.IsBodyTracked);
hold on;

for i = 14:16

X1 = [skeletonJoints(SkeletonConnectionMap(i,1),1,1); skeletonJoints(SkeletonConnectionMap(i,2),1,1)];
Y1 = [skeletonJoints(SkeletonConnectionMap(i,1),2,1), skeletonJoints(SkeletonConnectionMap(i,2),2,1)];
jointPos = depthMetaData.JointPositions(:,:,1);
handPos = jointPos(8,:);
disp("Robot Position Changed")
[jointX,jointY,jointZ]= transform2Local(handPos(1,1),handPos(1,2), handPos(1,3))

        disp("Human hand Position Changed")
        disp(handPos) 
        disp('numbers of body detected')
        disp(nBodies)  

%% Decision Making Algorithm 
% The System takes movements of left hand of a person infront of the RGB-D camera as input 
% to sent high level commands to the robot to pick and place the the bricks

if handPos(1,1) > 0.3 && Cyton.taskExecutionFlag==0
    
    Cyton.pickBrick1();             % This command is used to reach the position of first Brick
end

if handPos(1,3)<1 && Cyton.taskExecutionFlag==1
   
    Cyton.DropBrick1();             % This command is used to pick first Brick and place it on the  specified position on the wall ;
end

if handPos(1,1)>0.3 && Cyton.taskExecutionFlag==2
    
    Cyton.pickBrick2();             % This command is used to reach the position of Second Brick
end

if handPos(1,3)<1 && Cyton.taskExecutionFlag==3
    
    Cyton.DropBrick2();
end

if handPos(1,1)>0.3 && Cyton.taskExecutionFlag==4
    
    Cyton.pickBrick3();             % This command is used to reach the position of Third Brick
end

if handPos(1,3)<1 && Cyton.taskExecutionFlag==5
    
    Cyton.DropBrick3();
    
end
if Cyton.taskExecutionFlag==6
   disp('task executed') 
end

axis on;

 line(X1,Y1, 'LineWidth', 2, 'LineStyle', '-' , 'Marker', '+', 'Color', colors(1));
 rectangle('Position',[100 100 350 250],'LineWidth', 3,'EdgeColor',[1 0 0]);
   
end
hold off;

end

end
stop(depthVid);